
<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yushi Bai</title>
  <meta name="author" content="Yushi Bai">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="UTF-8">
  <meta http-equiv="Content-type" content="text/html; charset=UTF-8">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table
    style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:20%;max-width:60%">
                  <a href="images/photo.JPG"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/photo.JPG" class="hoverZoomLink"></a>
                </td>
                <td style="padding:2.5%;width:65%;vertical-align:middle">
                  <p>
                    <name>Yushi Bai 白雨石</name>
                  </p>
                  <p>I am a Fourth-year Ph.D. student at Tsinghua University, advised by Prof. <a href="http://keg.cs.tsinghua.edu.cn/persons/ljz/">Juanzi Li</a>. I reveived my B.E. in <a href="https://iiis.tsinghua.edu.cn/en/">Yao class</a> from Tsinghua University in 2022. I have been very fortunate to work with Prof. <a href="https://keg.cs.tsinghua.edu.cn/jietang/">Jie Tang</a>, Prof. <a href="https://nlp.csai.tsinghua.edu.cn/~lzy/">Zhiyuan Liu</a>, Prof.  <a href="https://cs.stanford.edu/people/jure/">Jure Leskovec</a>, and Prof. <a href="https://procaccia.info/">Ariel Procaccia</a>.
                  </p>
                  <p>
                    <a href="mailto:bys22@mails.tsinghua.edu.cn">Email</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=E6cma10AAAAJ">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/bys0318/">Github</a>&nbsp/&nbsp
                    <a href="#News">News</a>&nbsp/&nbsp
                    <a href="#SelectedPublications">Selected Publications</a>&nbsp/&nbsp
                    <a href="#Experience">Experience</a>&nbsp/&nbsp
                    <a href="#HonorsAwards">Honors & Awards</a>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr id="News">
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 20px;width:100%;vertical-align:middle" class="news-list">
                  <p><span class="news-date">[2025.09]</span>Check out our newest paper <span class="news-highlight">SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression</span>, a simple yet powerful recipe for training reasoning models that achieves both higher accuracy and fewer tokens.</p>
                  <p><span class="news-date">[2025.09]</span>Two papers accepted to <span class="news-highlight">NeurIPS 2025</span>, including one <span class="news-highlight">Spotlight</span>!</p>
                  <p><span class="news-date">[2025.08]</span><span class="news-highlight">GLM-4.5 Technical Report</span> is released! I am one of the core contributors for the post-training stage.</p>
                  <p><span class="news-date">[2025.05]</span>Three papers including <span class="news-highlight">LongBench v2</span> accepted to <span class="news-highlight">ACL 2025</span>! See you in Vienna!</p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr id="SelectedPublications">
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Selected Publications</heading>
                  <p>
                    My research interests primarily lie in large language models (LLMs), covering long-context LLM, LLM evaluation and safety, and multimodality. Recently, I have been focusing on designing more efficient and comprehensive LLM architectures that push beyond traditional architectures, particularly in long-context and agentic scenarios. Below are some of my lead works (see my <a href="https://scholar.google.com/citations?user=E6cma10AAAAJ">Google Scholar</a> for a complete list).
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Publications Container with Sidebar -->
          <div style="padding:0px 20px;">
            <div class="publications-container">
              <!-- Left Sidebar Navigation -->
              <div class="category-sidebar">
                <div class="category-nav-item active" onclick="showCategory('long-context')">
                  <div class="category-nav-title">Long-Context LLM<span class="category-nav-count">(6)</span></div>
                </div>
                <div class="category-nav-item" onclick="showCategory('llm-eval')">
                  <div class="category-nav-title">LLM Evaluation & Safety<span class="category-nav-count">(5)</span></div>
                </div>
                <div class="category-nav-item" onclick="showCategory('multimodal')">
                  <div class="category-nav-title">Multimodal<span class="category-nav-count">(5)</span></div>
                </div>
                <div class="category-nav-item" onclick="showCategory('kg-reasoning')">
                  <div class="category-nav-title">Knowledge Graph Reasoning<span class="category-nav-count">(3)</span></div>
                </div>
                <div class="category-nav-item" onclick="showCategory('fair-division')">
                  <div class="category-nav-title">Fair Division<span class="category-nav-count">(2)</span></div>
                </div>
              </div>

              <!-- Right Content Area -->
              <div class="publications-content">
                <!-- Long Context Language Models -->
                <div class="category-section active" id="long-context-section">
                  <h2 class="section-title">Long-Context LLM</h2>
              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <a href="images/longwriter-zero.png"><img src='images/longwriter-zero.png' width="100%"></a>
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning</papertitle>
                      <br>
                      Yuhao Wu*, <strong>Yushi Bai*</strong>,
                      Zhiqiang Hu, Roy Ka-Wei Lee, Juanzi Li
                      <br>
                      arxiv
                      <br>
                      [<a href="https://arxiv.org/abs/2506.18841">paper</a>]
                      [<a href="https://github.com/THUDM/LongWriter">code</a>]
                      <p></p>
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <a href="images/longbench2.png"><img src='images/longbench2.png' width="100%"></a>
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks</papertitle>
                      <br>
                      <strong>Yushi Bai*</strong>,
                      Shangqing Tu*, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li
                      <br>
                      ACL 2025
                      <br>
                      [<a href="https://arxiv.org/abs/2412.15204">paper</a>]
                      [<a href="https://longbench2.github.io">website</a>]
                      [<a href="https://github.com/THUDM/LongBench">code</a>]
                      <a href="https://resources.paperdigest.org/2025/09/most-influential-acl-papers-2025-09-version/" class="blue-button">
                        Top-15 Influential paper at ACL 25
                      </a>
                      <p></p>
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <a href="images/longwriter.JPEG"><img src='images/longwriter.JPEG' width="100%"></a>
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs</papertitle>
                      <br>
                      <strong>Yushi Bai</strong>,
                      Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li
                      <br>
                      ICLR 2025
                      <br>
                      [<a href="https://arxiv.org/abs/2408.07055">paper</a>]
                      [<a href="https://github.com/THUDM/LongWriter">code</a>]
                      <p></p>
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <a href="images/longcite.png"><img src='images/longcite.png' width="100%"></a>
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>LongCite: Enabling LLMs to Generate Fine-grained Citations in Long-context QA</papertitle>
                      <br>
                      Jiajie Zhang, <strong>Yushi Bai</strong>, Xin Lv, Wanjun Gu, Danqing Liu, Minhao Zou, Shulin Cao, Lei Hou, Yuxiao Dong, Ling Feng, Juanzi Li
                      <br>
                      EMNLP 2025
                      <br>
                      [<a href="https://arxiv.org/abs/2409.02897">paper</a>]
                      [<a href="https://github.com/THUDM/LongCite">code</a>]
                      <p></p>
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <a href="images/longalign.gif"><img src='images/longalign.gif' width="100%"></a>
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>LongAlign: A Recipe for Long Context Alignment of Large Language Models</papertitle>
                      <br>
                      <strong>Yushi Bai</strong>,
                      Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, Juanzi Li
                      <br>
                      EMNLP 2024
                      <br>
                      [<a href="https://arxiv.org/abs/2401.18058">paper</a>]
                      [<a href="https://github.com/THUDM/LongAlign">code</a>]
                      <p></p>
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <a href="images/longbench.gif"><img src='images/longbench.gif' width="100%"></a>
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding</papertitle>
                      <br>
                      <strong>Yushi Bai</strong>,
                      Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li
                      <br>
                      ACL 2024
                      <br>
                      [<a href="https://arxiv.org/abs/2308.14508">paper</a>]
                      [<a href="https://github.com/THUDM/LongBench">code</a>]
                      <a href="https://resources.paperdigest.org/2025/09/most-influential-acl-papers-2025-09-version/" class="blue-button">
                        Top-2 Influential paper at ACL 24
                      </a>
                      <p></p>
                    </td>
                  </tr>
                </tbody>
              </table>
                </div>

                <!-- LLM Evaluation -->
                <div class="category-section" id="llm-eval-section">
                  <h2 class="section-title">LLM Evaluation & Safety</h2>
              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <a href="images/auto-dataset.png"><img src='images/auto-dataset.png' width="100%"></a>
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>Automating dataset updates towards reliable and timely evaluation of large language models</papertitle>
                      <br>
                      Jiahao Ying*, Yixin Cao*, <strong>Yushi Bai</strong>, Qianru Sun, Bo Wang, Wei Tang, Zhaojun Ding, Yizhe Yang, Xuanjing Huang, Shuicheng Yan
                      <br>
                      NeurIPS 2024
                      <br>
                      [<a href="https://arxiv.org/abs/2402.11894">paper</a>]
                      [<a href="https://yingjiahao14.github.io/Automating-DatasetUpdates/">website</a>]
                      <p></p>
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <a href="images/privacy.png"><img src='images/privacy.png' width="100%"></a>
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>PrivacyMind: Large Language Models Can Be Contextual Privacy Protection Learners</papertitle>
                      <br>
                      Yijia Xiao, Yiqiao Jin, <strong>Yushi Bai</strong>, Yue Wu, Xianjun Yang, Xiao Luo, Wenchao Yu, Xujiang Zhao, Yanchi Liu, Quanquan Gu, Haifeng Chen, Wei Wang, Wei Cheng
                      <br>
                      EMNLP 2024
                      <br>
                      [<a href="https://arxiv.org/abs/2310.02469">paper</a>]
                      [<a href="https://github.com/Yijia-Xiao/PrivacyMind">code</a>]
                      <p></p>
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <a href="images/waterbench.png"><img src='images/waterbench.png' width="100%"></a>
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models</papertitle>
                      <br>
                      Shangqing Tu*, Yuliang Sun*, <strong>Yushi Bai</strong>, Jifan Yu, Lei Hou, Juanzi Li
                      <br>
                      ACL 2024
                      <br>
                      [<a href="https://arxiv.org/abs/2311.07138">paper</a>]
                      [<a href="https://github.com/THU-KEG/WaterBench">code</a>]
                      <p></p>
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <a href="images/kola.png"><img src='images/kola.png' width="100%"></a>
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>KoLA: Carefully Benchmarking World Knowledge of Large Language Models</papertitle>
                      <br>
                      Jifan Yu*, Xiaozhi Wang*, Shangqing Tu*, ...
                      <strong>Yushi Bai</strong>, ...
                      Bin Xu, Jie Tang, Juanzi Li
                      <br>
                      ICLR 2024
                      <br>
                      [<a href="https://arxiv.org/abs/2306.09296">paper</a>]
                      [<a href="https://kola.xlore.cn/">website</a>]
                      <p></p>
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <a href="images/examiner.png"><img src='images/examiner.png' width="100%"></a>
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>Benchmarking Foundation Models with Language-Model-as-an-Examiner</papertitle>
                      <br>
                      <strong>Yushi Bai*</strong>,
                      Jiahao Ying*, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, Lei Hou
                      <br>
                      NeurIPS 2023
                      <br>
                      [<a href="https://arxiv.org/abs/2306.04181">paper</a>]
                      [<a href="https://lmexam.com/">website</a>]
                      <p></p>
                    </td>
                  </tr>
                </tbody>
              </table>
                </div>

                <!-- Multimodal -->
                <div class="category-section" id="multimodal-section">
                  <h2 class="section-title">Multimodal</h2>
              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <a href="images/mmgeolm.png"><img src='images/mmgeolm.png' width="100%"></a>
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>MMGeoLM: Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models</papertitle>
                      <br>
                      Kai Sun*, <strong>Yushi Bai*</strong>, Zhen Yang, Jiajie Zhang, Ji Qi, Lei Hou, Juanzi Li
                      <br>
                      arxiv
                      <br>
                      [<a href="https://arxiv.org/abs/2505.20152">paper</a>]
                      [<a href="https://github.com/THU-KEG/MMGeoLM">code</a>]
                      <p></p>
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <a href="images/quicksviewer.png"><img src='images/quicksviewer.png' width="100%"></a>
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>Quicksviewer: An LMM for Efficient Video Understanding via Reinforced Compression of Video Cubes</papertitle>
                      <br>
                      Ji Qi, Yuan Yao, <strong>Yushi Bai</strong>, Bin Xu, Juanzi Li, Zhiyuan Liu, Tat-Seng Chua
                      <br>
                      arxiv
                      <br>
                      [<a href="https://arxiv.org/abs/2504.15270">paper</a>]
                      [<a href="https://t3bench.com/">website</a>]
                      <p></p>
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <a href="images/t3bench.png"><img src='images/t3bench.png' width="100%"></a>
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>T^3Bench: Benchmarking Current Progress in Text-to-3D Generation</papertitle>
                      <br>
                      Yuze He*, <strong>Yushi Bai*</strong>, Matthieu Lin, Wang Zhao, Yubin Hu, Jenny Sheng, Ran Yi, Juanzi Li, Yong-Jin Liu
                      <br>
                      arxiv
                      <br>
                      [<a href="https://arxiv.org/abs/2310.02977">paper</a>]
                      [<a href="https://t3bench.com/">website</a>]
                      <p></p>
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <a href="images/cogcom.png"><img src='images/cogcom.png' width="100%"></a>
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>CogCoM: A Visual Language Model with Chain-of-Manipulations Reasoning</papertitle>
                      <br>
                      Ji Qi, Ming Ding, Weihan Wang, <strong>Yushi Bai</strong>, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, Jie Tang
                      <br>
                      ICLR 2025
                      <br>
                      [<a href="https://openreview.net/forum?id=Fg0eo2AkST">paper</a>]
                      [<a href="https://github.com/THUDM/CogCoM">code</a>]
                      <p></p>
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <a href="images/mmmath.png"><img src='images/mmmath.png' width="100%"></a>
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>MM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation and Fine-grained Classification</papertitle>
                      <br>
                      Kai Sun*, <strong>Yushi Bai*</strong>, Ji Qi, Lei Hou, Juanzi Li
                      <br>
                      EMNLP 2024
                      <br>
                      [<a href="https://arxiv.org/abs/2404.05091">paper</a>]
                      [<a href="https://github.com/kge-sun/MM-Math">code</a>]
                      <p></p>
                    </td>
                  </tr>
                </tbody>
              </table>
                </div>

                <!-- Knowledge Graph Reasoning -->
                <div class="category-section" id="kg-reasoning-section">
                  <h2 class="section-title">Knowledge Graph Reasoning</h2>
              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <a href="images/qto.png"><img src='images/qto.png' width="100%"></a>
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>Answering Complex Logical Queries on Knowledge Graphs via Query Computation Tree Optimization</papertitle>
                      <br>
                      <strong>Yushi Bai</strong>,
                      Xin Lv, Juanzi Li, Lei Hou
                      <br>
                      ICML 2023
                      <br>
                      [<a href="https://arxiv.org/abs/2212.09567">paper</a>]
                      [<a href="https://github.com/bys0318/QTO">code</a>]
                      <p></p>
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <a href="images/squire.png"><img src='images/squire.png' width="100%"></a>
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>SQUIRE: A Sequence-to-sequence Framework for Multi-hop Knowledge Graph Reasoning</papertitle>
                      <br>
                      <strong>Yushi Bai</strong>,
                      Xin Lv, Juanzi Li, Lei Hou, Yincen Qu, Zelin Dai, Feiyu Xiong
                      <br>
                      EMNLP 2022 (Oral)
                      <br>
                      [<a href="https://arxiv.org/abs/2201.06206">paper</a>]
                      [<a href="https://github.com/bys0318/SQUIRE">code</a>]
                      <p></p>
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <a href="images/cone.png"><img src='images/cone.png' width="100%"></a>
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>Modeling heterogeneous hierarchies with relation-specific hyperbolic cones</papertitle>
                      <br>
                      <strong>Yushi Bai*</strong>,
                      Rex Ying*, Hongyu Ren, Jure Leskovec
                      <br>
                      NeurIPS 2021
                      <br>
                      [<a href="https://arxiv.org/abs/2110.14923">paper</a>]
                      [<a href="https://github.com/snap-stanford/ConE">code</a>]
                      <p></p>
                    </td>
                  </tr>
                </tbody>
              </table>
                </div>

                <!-- Fair Division -->
                <div class="category-section" id="fair-division-section">
                  <h2 class="section-title">Fair Division</h2>
              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <a href="images/asymmetric.png"><img src='images/asymmetric.png' width="100%"></a>
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>Envy-free and pareto-optimal allocations for agents with asymmetric random valuations</papertitle>
                      <br>
                      <strong>Yushi Bai</strong>,
                      Paul Gölz
                      <br>
                      IJCAI 2022
                      <br>
                      [<a href="https://arxiv.org/abs/2109.08971">paper</a>]
                      <p></p>
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                      <a href="images/smoothed.png"><img src='images/smoothed.png' width="100%"></a>
                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle">
                      <papertitle>Fair allocations for smoothed utilities</papertitle>
                      <br>
                      <strong>Yushi Bai</strong>,
                      Uriel Feige, Paul Gölz, Ariel Procaccia
                      <br>
                      ACM EC 2022
                      <br>
                      [<a href="https://dl.acm.org/doi/abs/10.1145/3490486.3538285">paper</a>]
                      <p></p>
                    </td>
                  </tr>
                </tbody>
              </table>
                </div>
              </div>
            </div>
          </div>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr id="Experience">
                <td>
                  <heading>Experience</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="10">
            <tbody>
              <tr>
                <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img
                    src="images/thu.png" , width="80%"></td>
                <td width="80%" valign="center">
                  <b>Tsinghua University</b>
                  <br> 2022.08 - Present <br>
                  <br> <b>Ph.D student in Computer Science and Technology</b>
                  <br> Advisor: Prof. <a href="http://keg.cs.tsinghua.edu.cn/persons/ljz/">Juanzi Li</a>
                </td>
              </tr>
              <tr>
                <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img
                    src="images/harvard.png" , width="80%"></td>
                <td width="100%" valign="center">
                  <b>Harvard University</b>
                  <br> 2021.02-2021.09
                  <br>
                  <br> <b>Visiting Research Student</b>
                  <br> Advisor: Prof. <a href="https://procaccia.info/">Ariel Procaccia</a>
                </td>
              </tr>
              <tr>
                <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img
                    src="images/stanford.png" , width="80%"></td>
                <td width="100%" valign="center">
                  <b>Stanford University</b>
                  <br> 2020.06-2020.10
                  <br>
                  <br> <b>Visiting Research Student</b>
                  <br> Advisor: Prof. <a href="https://cs.stanford.edu/people/jure/index.html">Jure Leskovec</a>
                </td>
              </tr>
              <tr>
                <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img
                    src="images/thu.png" , width="80%"></td>
                <td width="80%" valign="center">
                  <b>Tsinghua University</b>
                  <br> 2018.08 - 2022.07 <br>
                  <br> <b>B.Eng. in Yao Class</b>
                </td>
              </tr>


            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr id="HonorsAwards">
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Honors & Awards</heading>
                  <ul>
                    <li>National Scholarship, 2024</li>
                    <li>Outstanding Undergraduate in Beijing, 2022</li>
                    <li>Yao Award, 2021</li>
                    <li>Tsinghua Comprehensive Excellence Scholarship, 2019, 2020, 2021, 2023</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    Template adapted from <a href="https://jonbarron.info/">Jon Barron</a>.
                    <script type='text/javascript' id='clustrmaps'
                      src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=200&t=n&d=FYhBoxLDEaFAxdfRzk5TuchYOBGrnSa98Ky59EkEEpY'>
                      </script>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>

  <script>
    function showCategory(categoryId) {
      // Hide all category sections
      const sections = document.querySelectorAll('.category-section');
      sections.forEach(section => {
        section.classList.remove('active');
      });
      
      // Remove active class from all nav items
      const navItems = document.querySelectorAll('.category-nav-item');
      navItems.forEach(item => {
        item.classList.remove('active');
      });
      
      // Show the selected category section
      const selectedSection = document.getElementById(categoryId + '-section');
      if (selectedSection) {
        selectedSection.classList.add('active');
      }
      
      // Add active class to the clicked nav item
      event.currentTarget.classList.add('active');
    }
  </script>
</body>

</html>